# Gemma LoRA-based MoE

## ğŸ¯ ëª©í‘œ
Gemma-3-4B ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ **LoRA ê¸°ë°˜ Mixture of Experts (MoE)** ì‹œìŠ¤í…œ êµ¬ì¶•

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### MoE êµ¬ì¡°
ê° Transformer ë ˆì´ì–´ë§ˆë‹¤:
- **Expert 0**: Original MLP (freeze)
- **Expert 1**: Original MLP + Medical LoRA
- **Expert 2**: Original MLP + Law LoRA  
- **Expert 3**: Original MLP + Math LoRA
- **Expert 4**: Original MLP + Code LoRA
- **Router**: Top-1 routing (í•™ìŠµ ëŒ€ìƒ)

### ì¥ì 
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ 0.48%ë§Œ í›ˆë ¨
- **í™•ì¥ì„±**: ìƒˆë¡œìš´ ë„ë©”ì¸ ì¶”ê°€ ìš©ì´
- **ì„±ëŠ¥**: ë„ë©”ì¸ë³„ ì „ë¬¸í™”ëœ ì‘ë‹µ

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
gemma-moe/
â”œâ”€â”€ train_domain_models.py    # ë„ë©”ì¸ë³„ LoRA í›ˆë ¨
â”œâ”€â”€ train_moe_router.py       # MoE ë¼ìš°í„° í›ˆë ¨  
â”œâ”€â”€ moe_architecture.py       # MoE ì•„í‚¤í…ì²˜ êµ¬í˜„
â”œâ”€â”€ dataset.py               # ë°ì´í„°ì…‹ ë¡œë”
â”œâ”€â”€ utils.py                 # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
â”œâ”€â”€ run_full_moe_pipeline.sh # ì „ì²´ íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ requirements.txt         # ì˜ì¡´ì„±
â”œâ”€â”€ data/                    # í›ˆë ¨ ë°ì´í„°
â”‚   â”œâ”€â”€ medical/
â”‚   â”œâ”€â”€ law/
â”‚   â”œâ”€â”€ math/
â”‚   â””â”€â”€ code/
â””â”€â”€ domain_models/           # í›ˆë ¨ëœ LoRA ì–´ëŒ‘í„°ë“¤
    â”œâ”€â”€ medical/
    â”œâ”€â”€ law/
    â”œâ”€â”€ math/
    â””â”€â”€ code/
```

## ğŸš€ ì‚¬ìš©ë²•

### 1. í™˜ê²½ ì„¤ì •
```bash
pip install -r requirements.txt
```

### 2. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
```bash
python download_data.py
```

### 3. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
```bash
./run_full_moe_pipeline.sh
```

### 4. ê°œë³„ ë„ë©”ì¸ í›ˆë ¨
```bash
python train_domain_models.py \
    --domain medical \
    --model google/gemma-3-4b-it \
    --epochs 3 \
    --batch_size 4 \
    --lora_r 16 \
    --lora_alpha 32 \
    --fp16
```

### 5. MoE ë¼ìš°í„° í›ˆë ¨
```bash
python train_moe_router.py \
    --base_model google/gemma-3-4b-it \
    --domain_adapter_paths ./domain_models/medical/final_adapter ./domain_models/law/final_adapter ./domain_models/math/final_adapter ./domain_models/code/final_adapter \
    --domains medical law math code
```

## âš™ï¸ ì£¼ìš” íŒŒë¼ë¯¸í„°

### LoRA ì„¤ì •
- **lora_r**: 16 (LoRA rank)
- **lora_alpha**: 32 (LoRA scaling factor)
- **lora_dropout**: 0.1
- **target_modules**: gate_proj, up_proj, down_proj

### í›ˆë ¨ ì„¤ì •
- **batch_size**: 4
- **learning_rate**: 2e-4
- **max_length**: 1024
- **fp16**: True

## ğŸ“Š ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
- **GPU**: NVIDIA RTX A6000 (48GB) ê¶Œì¥
- **ëª¨ë¸ ë¡œë”©**: ~8GB
- **LoRA í›ˆë ¨**: ~12GB ì´ ì‚¬ìš©
- **í›ˆë ¨ íŒŒë¼ë¯¸í„°**: 20.9M / 4.3B (0.48%)

## ğŸ¯ ë„ë©”ì¸
1. **Medical**: ì˜ë£Œ QA (MedMCQA)
2. **Law**: ë²•ë¥  QA (LegalBench)
3. **Math**: ìˆ˜í•™ ë¬¸ì œ (GSM8K)
4. **Code**: ì½”ë”© ë¬¸ì œ (CodeXGLUE)

## ğŸ” íŠ¹ì§•
- **Parameter Efficient**: LoRA ê¸°ë°˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- **Modular Design**: ë„ë©”ì¸ë³„ ë…ë¦½ì  í›ˆë ¨
- **Top-1 Routing**: ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì „ë¬¸ê°€ ì„ íƒ
- **Load Balancing**: ì „ë¬¸ê°€ ê°„ ê· ë“±í•œ ì‚¬ìš©ë¥  ìœ ë„