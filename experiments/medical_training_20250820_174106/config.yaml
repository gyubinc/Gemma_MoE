domain_configs:
  code:
    eval_samples: 1000
    max_samples: null
  law:
    eval_samples: 1000
    max_samples: null
  math:
    eval_samples: 500
    max_samples: null
  medical:
    eval_samples: 1000
    max_samples: null
domains:
- medical
- law
- math
- code
lora:
  alpha: 64
  bias: none
  dropout: 0.1
  r: 32
  target_modules:
  - gate_proj
  - up_proj
  - down_proj
  task_type: CAUSAL_LM
model:
  name: Qwen/Qwen3-4B-Instruct-2507
  torch_dtype: float16
  trust_remote_code: true
moe:
  batch_size: 2
  eval_steps: 200
  evaluation_strategy: steps
  expert_capacity: 64
  gradient_accumulation_steps: 16
  learning_rate: 1e-4
  load_balancing_loss_weight: 0.01
  logging_steps: 5
  lr_scheduler_type: cosine
  num_epochs: 2
  num_experts: 4
  router_type: top1
  save_steps: 200
  save_total_limit: 3
  warmup_ratio: 0.1
  weight_decay: 0.01
system:
  cleanup_checkpoints: true
  device_map: auto
  gpu_memory_gb: 46
  keep_last_checkpoints: 2
  max_memory_mb: 44000
  mixed_precision: fp16
  output_dir: ./domain_models
  seed: 42
training:
  bf16: false
  dataloader_num_workers: 4
  effective_batch_size: 32
  eval_steps: 1000
  fp16: true
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  learning_rate: 5e-4
  logging_steps: 100
  lr_scheduler_type: cosine
  max_grad_norm: 1.0
  max_length: 256
  max_steps: null
  num_epochs: 1
  per_device_batch_size: 32
  remove_unused_columns: false
  report_to: []
  save_steps: 1000
  save_total_limit: 2
  warmup_ratio: 0.1
  weight_decay: 0.01
